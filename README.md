# Описание содержимого файлов и директорий

* Директория **Realization** содержит программу, реализующую различные способы заполнения пропусков.
	* **Glass Dataset Handling** представляет собой ipynb файл, в котором реализовано создание пропусков в данных, их последующее заполнение, решение задачи классификации для оценки качества заполнения. В качестве метрики качества используется **F1-мера**. Это - **файл, запускающий программу**.
    * **Pima-Indians-Diabetes-Dataset** представляет собой ipynb файл, в котором первоначально имелись пропуски. Они были заполнены различными методами, впоследствии была решена задача классификации для каждого варианта заполнения пропусков и получены значения  **F1-меры**.

	* **mv_tools.py** - содержит инструменты для создания пропусков разных типов (**MCAR**, **MAR**, **MNAR**) в данных, а также инструменты для заполнения пропусков.

	* **solver.py** - содержит функции для решения задачи классификации и последующего вывода метрик качества в форме таблицы (DataFrame).

	* **Realization/Datasets** содержит наборы данных, для которых применяются алгоритмы заполнения пропусков. 

* Директория **Theory** включает в себя ipynb-файл, в нем описан вывод формул для ЕМ-алгоритма.

# Описание алгоритмов обработки пропущенных значений

* **Mean Imputation**: идея этого алгоритма состоит в том, чтобы для каждого признака, содержащего пропущенные значения, вычислять среднее значение наблюдаемых данных, и заполнять им все пропуски. Такой подход работает лишь при наличии большого набора наблюдений с малой долей пропусков. В противном случае выборка становится нерезепрентативной.

* **Stochastic Linear Regression Imputation**: На основе наблюдаемых данных (наблюдений, для которых известны значения всех признаков) вычисляются веса линейной регрессии. Далее, на основе модели линейной регрессии предсказываются вероятные пропущенные значения, к полученным прогнозам добавляется гауссовский шум, чье математическое ожидание равно нулю, а дисперсия равна остаточной дисперсии модели.

* **EM-Algorithm**:
    * Используется следующее правдоподобие:
      $$ Q(\theta|\theta^{(t)}) =  -\frac{n}{2} \sum_{i=1}^n [ \log{|2\pi\Sigma|} + tr([\Sigma^{-1}]_{M_iM_i}\Sigma_{M_iM_i|O_i}^{(t)}) (x_i^{(t)}-\mu)^{T}\Sigma^{-1}(x_i^{(t)}-\mu) ] $$
	* Задаются начальные оценки параметров $\theta=(\mu, \Sigma)$ на основе наблюдаемых данных.
	* До тех пор, пока изменение оценок параметров не станет слишком малым, или до тех пор пока не будет достигнуто максимальное число итераций итеративно выполнять E-шаг и M-шаг.
		* На Е-шаге находим латентные переменные путем заполнения пропущенных значений условным математическим ожиданием. 
		* На М-шаге по заполненному набору данных, и с учетом условной ковариационной матрицы, мы вычисляем новые оценки параметров.
	* Заполнить пропущенные данные в наборе с использованием условного математического ожидания.

	
* **Bootstrap Imputation**: для каждого признака, содержащего пропущенные значения, на основе множества наблюдаемых значений этого признака, создается множество с возвращением, чья длина равна числу пропусков для данного признака. С помощью этого множества пропуски и заполняются.

* **Итеративное заполнение**: sklearn.IterativeImputer предлагает свой вариант заполнения пропусков. Реализуется следующим образом: вычисляется начальная оценка пропущенных значений (для каждого признака пропуски заполняются выборочным средним). Затем для каждого признака вычисляются значения пропусков с помощью алгоритма машинного обучения на основе значений (включая восстановленные) остальных признаков. Таким образом формируется новая оценка полного набора данных.
Новые оценки итеративно строятся до сходимости, либо до достижения определенного числа итераций.

* **KNN Imputation**: используется метод ближайших соседей, выбранная метрика - **nan_euclidean**. $$dist(x,y)=\sqrt{\frac{N}{M}\sum_{i=1}^M {(x_{F_i}-y_{F_i})^2}},$$ где **N** - число признаков, **M** - число таких признаков **F**, чьи значения не пропущены для обоих объектов (x и y). Предположим, что для объекта **i** пропущено значение признака **j**, тогда значение признака **j** вычисляется как среднее (либо как средне-взвешенное) значение соседей объекта **i** не имеющих пропуска в признаке **j**.

# Описание последовательности обработки наборов данных

* Загрузка датасета из файла.

* Для каждого признака реализуется тест Шапиро-Уилка (проверка нормальности распределения).

* Определяется число различных классов целевой переменной.

* На основе набора данных создаются четыре его копии:
	* Копия с пропусками типа **MCAR**, на практике это означет, что пропуски не зависят как от наблюдаемых, так и от самих пропущенных данных.
	* Копия с пропусками типа **MAR**, на практике это означает, что пропуски зависят от наблюдаемых данных.
	* Копия с пропусками типа **MNAR(тип 1)**, на практике это означает, что пропуски зависят от индексов, в которых находятся. Для каждого индекса вероятность пропуска определяется бета-распределением.
	* Копия с пропусками типа **MNAR(тип 2)**, на практике это означает, что пропуски зависят от индексов, в которых находятся. Для каждого индекса вероятность пропуска определяется нормальным распределением.

* К каждой из этих копий применяются семь методов заполнения указанных выше, таким образом создается 28 набора данных на основе изначального.

* Для каждого из этих наборов данных решается задача классификации с помощью **LightGBM**.

* Задача классификации решается также для набор данных с пропусками, но с использованием **CatBoost**, **LightGBM**.

* Во всех указанных выше случаев используется кросс-валидация для усреднения результатов, во всех указанных случаях происходит вычисление метрики **F1-мера**.

* Выводятся таблица (pandas.DataFrame): для пропусков типа **MCAR**, для пропусков типа **MAR**, для пропусков типа **MNAR**, распределенных в соответствии с бета-распределением, для пропусков типа **MNAR**, распределенных в соответствии с нормальным распределением. Каждая таблица имеет размер 9x4, строки соответствуют способу взаимодействия с пропусками, столбцы - метрикам качества.